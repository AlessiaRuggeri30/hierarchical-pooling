{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hierarchical pooling.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DdZPc3F7qBVm","colab_type":"text"},"source":["# Geometric Deep Learning Project - Towards Sparse Hierarchical Graph Classifiers\n","\n","*Alessia Ruggeri*\n","\n","### Implementation of the paper *Towards Sparse Hierarchical Graph Classifiers* tested on Enzymes, Proteins and D&D biological datasets using Tensorflow 2.0."]},{"cell_type":"code","metadata":{"id":"21c39ObKmnPF","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu==2.0.0-alpha0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zesQgjExrN0k","colab_type":"code","colab":{}},"source":["import os,sys,inspect\n","import networkx as nx\n","import numpy as np\n","import scipy\n","import scipy.sparse as sp\n","import matplotlib.pyplot as plt\n","import time\n","import math\n","\n","import tensorflow as tf\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Layer, Input, Dense, Flatten, Activation, Dropout, ReLU\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras import Model\n","from sklearn.utils import shuffle\n","\n","from load_data import read_graphfile\n","\n","np.random.seed(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OU99Vk_Cuuw1","colab_type":"code","colab":{}},"source":["### Unzip datasets folders\n","\n","!unzip -o data_ENZYMES.zip\n","!unzip -o data_PROTEINS.zip\n","# !unzip -o data_DD.zip\n","# !unzip -o data_COLLAB.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhJQXkB4rN6E","colab_type":"code","colab":{}},"source":["### Load datasets\n","\n","print(\"\\nLoading ENZYMES...\")\n","graphs_ENZYMES = read_graphfile(datadir=\"data_ENZYMES\", dataname=\"ENZYMES\", max_nodes=None)\n","\n","print(\"\\nLoading PROTEINS...\")\n","graphs_PROTEINS = read_graphfile(datadir=\"data_PROTEINS\", dataname=\"PROTEINS\", max_nodes=None)\n","\n","# print(\"\\nLoading DD...\")\n","# graphs_DD = read_graphfile(datadir=\"data_DD\", dataname=\"DD\", max_nodes=None)\n","\n","# print(\"\\nLoading COLLAB...\")\n","# graphs_COLLAB = read_graphfile(datadir=\"data_COLLAB\", dataname=\"COLLAB\", max_nodes=None)\n","\n","print(\"\\nDone.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o6SUikFsKB2x","colab_type":"code","colab":{}},"source":["### Generic functions\n","\n","def get_numberof_features(dataset_name):\n","  if dataset_name == \"ENZYMES\":\n","    return 18\n","  elif dataset_name == \"PROTEINS\":\n","    return 1\n","  return None\n","\n","def preprocess_features(features):\n","  '''Row-normalize feature matrix and convert it to dense representation'''\n","  rowsum = np.array(features.sum(1))\n","  r_inv = np.power(rowsum, -1).flatten()\n","  r_inv[np.isinf(r_inv)] = 0.\n","  r_mat_inv = sp.diags(r_inv)\n","  features = r_mat_inv.dot(features)\n","  return features\n","\n","def get_node_features_matrix(graph):\n","  '''It returns the node feature matrix of the graph with already preprocessed features'''\n","  Xdict = nx.get_node_attributes(graph, 'feat')\n","  X = np.array([Xdict[i] for i in range(nx.number_of_nodes(graph))])\n","  X = preprocess_features(X)\n","  X = X.astype(np.float32)\n","  return X\n","\n","def get_adjacency_matrix(graph):\n","  '''It returns the adjacency matrix of the graph with inserted self-loops'''\n","  A = nx.adjacency_matrix(graph)\n","  A = np.array(A + np.eye(A.shape[0]))\n","  A = sp.coo_matrix(A.astype(np.float32))\n","  return A\n","\n","def get_normalization_matrix(A):\n","  '''It returns the normalized adjacency matrix of the graph'''\n","  degrees = np.array(np.sum(A.todense(), axis=1)).flatten()\n","  degrees = np.power(degrees, -1)\n","  degrees[np.isinf(degrees)] = 0\n","  degrees = degrees.astype(np.float32)\n","  D = sp.diags(degrees, offsets=0).tocoo()\n","  return D\n","\n","def get_normalized_adjacency_matrix(A):\n","  D = get_normalization_matrix(A)\n","  A_norm = D @ A\n","  return A_norm.tocoo()\n","\n","def get_graphs_labels(dataset):\n","  '''It returns the class labels of all the graphs in the dataset'''\n","  labels = []\n","  for graph in dataset:\n","    labels.append(graph.graph['label'])\n","  labels = np.array([[labels[i]] for i in range(len(labels))])\n","  return labels\n","\n","def dot(x, y, sparse=False):\n","  '''Wrapper for tf.matmul (sparse vs dense)'''\n","  if sparse:\n","      res = tf.sparse.sparse_dense_matmul(x, y)\n","  else:\n","      res = tf.matmul(x, y)\n","  return res\n","  \n","def convert_sparse_matrix_to_sparse_tensor(coo):\n","  indices = np.transpose(np.array([coo.row, coo.col]))\n","  return tf.SparseTensor(indices, coo.data.astype(np.float32), coo.shape)\n","\n","def convert_nparray_to_sparse_tensor(nparray):\n","  tf_tensor = tf.constant(nparray)\n","  idx = tf.where(tf.not_equal(tf_tensor, 0))\n","  sparse_tensor = tf.SparseTensor(idx, tf.gather_nd(tf_tensor, idx), tf_tensor.get_shape())\n","  return sparse_tensor\n","\n","def convert_tf_tensor_to_sparse_tensor(tf_tensor):\n","  idx = tf.where(tf.not_equal(tf_tensor, 0))\n","  sparse_tensor = tf.SparseTensor(idx, tf.gather_nd(tf_tensor, idx), tf_tensor.get_shape())\n","  return sparse_tensor\n","\n","def one_hot_encoding(data, n_classes):\n","    '''It one-hot encode data'''\n","    targets = np.array(data).reshape(-1)\n","    targets = np.eye(n_classes)[targets]\n","    return targets\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"paqyqK-Zf_j_","colab_type":"code","colab":{}},"source":["### Execution functions\n","\n","def convert_dataset_to_lists(dataset):\n","  feat = []\n","  adj = []\n","  for graph in dataset:\n","    X = get_node_features_matrix(graph)\n","    A = get_adjacency_matrix(graph)\n","    A_norm = get_normalized_adjacency_matrix(A)\n","    feat.append(X)\n","    adj.append(A_norm)\n","  return feat, adj\n","\n","def create_train_val_test_sets(dataset):\n","  labels = get_graphs_labels(dataset)\n","  n_classes=len(np.unique(labels))\n","\n","  dataset, labels = shuffle(dataset, labels)\n","\n","  train_n = len(dataset)//100 * 80\n","  val_n = len(dataset)//100 * 15\n","\n","  (x_val, y_val) = dataset[0:val_n], labels[0:val_n]\n","  (x_train, y_train) = dataset[val_n:train_n], labels[val_n:train_n]\n","  (x_test, y_test) = dataset[train_n:], labels[train_n:]\n","\n","  X_train, A_train = convert_dataset_to_lists(x_train)\n","  X_val, A_val = convert_dataset_to_lists(x_val)\n","  X_test, A_test = convert_dataset_to_lists(x_test)\n","\n","  y_train = one_hot_encoding(y_train, n_classes)\n","  y_val = one_hot_encoding(y_val, n_classes)\n","  y_test = one_hot_encoding(y_test, n_classes)\n","  \n","  print('train:', len(x_train))\n","  print('val:', len(x_val))\n","  print('test:', len(x_test))\n","  \n","  return [X_train, A_train, y_train], [X_val, A_val, y_val], [X_test, A_test, y_test], n_classes\n","\n","def create_batch_elements(X, A):\n","  '''It takes X and A lists and creates respective stack of nodes, block diagonal adjacency matrix and graph idx array'''\n","  X_stack = np.vstack(X)\n","  A_diag = sp.block_diag(A)\n","  A_diag = convert_sparse_matrix_to_sparse_tensor(A_diag)\n","  A_diag = tf.sparse.reorder(A_diag)\n","  n_nodes = np.array([a.shape[0] for a in A])\n","  graph_idx = np.repeat(np.arange(len(n_nodes)), n_nodes)\n","  return X_stack, A_diag, graph_idx\n","\n","def batch_generator(data, batch_size=32):\n","  '''It takes a list of arrays or matrices and it yields batches of given size'''\n","  len_data = len(data[0])\n","  batches_per_epoch = math.ceil(len_data/batch_size)\n","  for batch in range(batches_per_epoch):\n","    start = batch * batch_size\n","    end = min(start+batch_size, len_data)\n","    out = [item[start:end] for item in data]\n","    yield out\n","\n","# @tf.function\n","def train_step(data, labels):\n","  with tf.GradientTape() as tape:\n","    predictions = model(data)\n","    loss = loss_object(labels, predictions)\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","  train_loss(loss)\n","  train_accuracy(labels, predictions)\n","  \n","# @tf.function\n","def val_step(data, labels):\n","  predictions = model(data)\n","  t_loss = loss_object(labels, predictions)\n","\n","  val_loss(t_loss)\n","  val_accuracy(labels, predictions)\n","  \n","# @tf.function\n","def test_step(data, labels):\n","  predictions = model(data)\n","  t_loss = loss_object(labels, predictions)\n","\n","  test_loss(t_loss)\n","  test_accuracy(labels, predictions)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZTbbrtxXKSe","colab_type":"code","colab":{}},"source":["### Define the layers\n","\n","\n","class Convolutional(Layer):\n","  \n","  def __init__(self, F, F_1, regularizer, **kwargs):\n","    self.F = F\n","    self.F_1 = F_1\n","    self.regularizer = regularizer\n","    super(Convolutional, self).__init__(**kwargs)\n","\n","  def build(self, input_shape):\n","    self.W1 = self.add_weight(name='W1', \n","                             shape=(self.F, F_1),\n","                             initializer='he_normal',\n","                             regularizer=self.regularizer,\n","                             trainable=True)\n","\n","    self.W2 = self.add_weight(name='W2', \n","                             shape=(self.F, F_1),\n","                             initializer='he_normal',\n","                             regularizer=self.regularizer,\n","                             trainable=True)\n","\n","    super(Convolutional, self).build(input_shape)  # Be sure to call this at the end\n","    \n","  def kernel(self, X, A):\n","    \n","    res = dot(A, X, sparse=True)\n","#     res = (N, F) -> AX\n","    res = dot(res, self.W1, sparse=False)\n","#     res = (N, F_1) -> AXW1\n","    skip_connection = dot(X, self.W2, sparse=False)\n","#     skip_connection = (N, F_1) -> XW2\n","    \n","    res = tf.math.add(res, skip_connection)\n","    res = ReLU()(res)\n","#     res = (N, F_1) -> sigma(AXW1 + XW2)\n","    \n","    return res\n","   \n","  def call(self, inputs):\n","    X = inputs[0]\n","    A = inputs[1]\n","\n","    res = self.kernel(X, A)\n","\n","    return res\n","\n","  def compute_output_shape(self, input_shape):\n","    return (input_shape[0][0], self.F_1)\n","\n","  \n","class GlobalAvgPooling(Layer):\n","  \n","  def __init__(self, **kwargs):\n","    super(GlobalAvgPooling, self).__init__(**kwargs)\n","    \n","  def build(self, input_shape):\n","    super(GlobalAvgPooling, self).build(input_shape)  # Be sure to call this at the end\n","  \n","  def call(self, inputs):\n","    nodes_feat = inputs[0]\n","    idx = inputs[1]\n","    \n","    res = tf.math.segment_mean(nodes_feat, idx)\n","    \n","    return res\n","  \n","  def compute_output_shape(self, input_shape):\n","    return (len(np.unique(idx)), self.F_1)\n","  \n","\n","class GlobalMaxPooling(Layer):\n","  \n","  def __init__(self, **kwargs):\n","    super(GlobalMaxPooling, self).__init__(**kwargs)\n","    \n","  def build(self, input_shape):\n","    super(GlobalMaxPooling, self).build(input_shape)  # Be sure to call this at the end\n","  \n","  def call(self, inputs):\n","    nodes_feat = inputs[0]\n","    idx = inputs[1]\n","    \n","    res = tf.math.segment_max(nodes_feat, idx)\n","    \n","    return res\n","  \n","  def compute_output_shape(self, input_shape):\n","    return (len(np.unique(idx)), self.F_1)\n","  \n","  \n","class GlobalSumPooling(Layer):\n","  \n","  def __init__(self, **kwargs):\n","    super(GlobalSumPooling, self).__init__(**kwargs)\n","    \n","  def build(self, input_shape):\n","    super(GlobalSumPooling, self).build(input_shape)  # Be sure to call this at the end\n","  \n","  def call(self, inputs):\n","    nodes_feat = inputs[0]\n","    idx = inputs[1]\n","    \n","    res = tf.math.segment_sum(nodes_feat, idx)\n","    \n","    return res\n","  \n","  def compute_output_shape(self, input_shape):\n","    return (len(np.unique(idx)), self.F_1)\n","  \n","  \n","class HierarchicalPooling(Layer):\n","  \n","  def __init__(self, F, pooling_ratio, regularizer, **kwargs):\n","    self.F = F\n","    self.k = pooling_ratio\n","    self.regularizer = regularizer\n","    super(HierarchicalPooling, self).__init__(**kwargs)\n","  \n","  def build(self, input_shape):\n","    \n","    self.p = self.add_weight(name='p', \n","                             shape=(self.F, 1),\n","                             initializer='he_normal',\n","                             regularizer=self.regularizer,\n","                             trainable=True)\n","    \n","    super(HierarchicalPooling, self).build(input_shape)  # Be sure to call this at the end\n","  \n","  def call(self, inputs):\n","    X = inputs[0]\n","    A = inputs[1]\n","    idx = inputs[2]\n","    A = tf.sparse.to_dense(A)\n","    \n","#   Generate projection scores\n","    p_norm = tf.math.l2_normalize(self.p, axis=0)\n","    proj = tf.math.divide(self.p, p_norm)\n","    y = dot(X, proj, sparse=False)\n","#     y = (N, 1)\n","    \n","    feat = []\n","    adj = []\n","    indices = []\n","    for g in range(tf.size(tf.unique(idx)[0])):\n","#     Select only nodes of one graph\n","      gg = tf.reshape(tf.where(tf.equal(idx, g)), [-1])\n","      X_g = tf.gather(X, indices=gg)\n","      A_g = tf.gather(tf.gather(A, gg, axis=0),  gg, axis=1)\n","      idx_g = tf.gather(idx, gg, axis=0)\n","      y_g = tf.gather(y, gg, axis=0)\n","      \n","#     Generate indices of nodes to keep for that graph\n","      N = X_g.shape[0]\n","      if self.k < 1:\n","        to_keep = tf.cast(tf.math.ceil(N*self.k), dtype=tf.int32)\n","      else:\n","        to_keep = N\n","      i = tf.argsort(y_g, axis=0, direction='DESCENDING')\n","      i = tf.reshape(i, [-1])[:to_keep]\n","      \n","#     Filter X, A and idx of the graph using i to keep only nodes with higher projection score\n","      X_g = tf.gather(X_g, indices=i)\n","      A_g = tf.gather(tf.gather(A_g, i, axis=0),  i, axis=1)\n","      idx_g = tf.gather(idx_g, i, axis=0)\n","      \n","      y_tilde = tf.tanh(tf.gather(y_g, i, axis=0))\n","      X_tilde = tf.multiply(X_g, y_tilde)\n","      \n","      feat.append(X_tilde)\n","      adj.append(A_g)\n","      indices.append(idx_g)\n","    \n","#   Rebuild X, A and idx for the entire batch\n","    X_stack = tf.concat(feat, axis=0)\n","    A_diag = sp.block_diag(adj)\n","    A_diag = convert_sparse_matrix_to_sparse_tensor(A_diag)\n","    A_diag = tf.sparse.reorder(A_diag)\n","    idx_seq = tf.concat(indices, axis=-1)\n","    \n","    return X_stack, A_diag, idx_seq\n","\n","\n","class MyModel(Model):\n","  def __init__(self, F, F_1, pooling_ratio, n_classes, dropout, reg):\n","    super(MyModel, self).__init__()\n","    \n","    self.conv1 = Convolutional(F=F, F_1=F_1, regularizer=reg)\n","    self.hpool1 = HierarchicalPooling(F=F_1, pooling_ratio=pooling_ratio, regularizer=reg)\n","    self.avgpool1 = GlobalAvgPooling()\n","    self.maxpool1 = GlobalMaxPooling()\n","    self.conv2 = Convolutional(F=F_1, F_1=F_1, regularizer=reg)\n","    self.hpool2 = HierarchicalPooling(F=F_1, pooling_ratio=pooling_ratio, regularizer=reg)\n","    self.avgpool2 = GlobalAvgPooling()\n","    self.maxpool2 = GlobalMaxPooling()\n","    self.conv3 = Convolutional(F=F_1, F_1=F_1, regularizer=reg)\n","    self.hpool3 = HierarchicalPooling(F=F_1, pooling_ratio=pooling_ratio, regularizer=reg)\n","    self.avgpool3 = GlobalAvgPooling()\n","    self.maxpool3 = GlobalMaxPooling()\n","    self.sumpool = GlobalSumPooling()\n","    self.drop = Dropout(dropout)\n","    self.dense = Dense(n_classes, activation='softmax', kernel_regularizer=reg)\n","\n","  def call(self, inputs):\n","    X = inputs[0]\n","    A = inputs[1]\n","    idx = inputs[2]\n","    \n","#   First conv-pool block\n","    X_conv1 = self.conv1([X, A])\n","    X_hpool1, A_hpool1, idx_hpool1 = self.hpool1([X_conv1, A, idx])\n","    avg1 = self.avgpool1([X_hpool1, idx_hpool1])\n","    max1 = self.maxpool1([X_hpool1, idx_hpool1])\n","    s1 = tf.concat([avg1, max1], axis=1)\n","    idx1 = tf.range(s1.shape[0])\n","#   Second conv-pool block\n","    X_conv2 = self.conv2([X_hpool1, A_hpool1])\n","    X_hpool2, A_hpool2, idx_hpool2 = self.hpool2([X_conv2, A_hpool1, idx_hpool1])\n","    avg2 = self.avgpool2([X_hpool2, idx_hpool2])\n","    max2 = self.maxpool1([X_hpool2, idx_hpool2])\n","    s2 = tf.concat([avg2, max2], axis=1)\n","    idx2 = tf.range(s2.shape[0])\n","#   Third conv-pool block\n","    X_conv3 = self.conv3([X_hpool2, A_hpool2])\n","    X_hpool3, A_hpool3, idx_hpool3 = self.hpool3([X_conv3, A_hpool2, idx_hpool2])\n","    avg3 = self.avgpool3([X_hpool3, idx_hpool3])\n","    max3 = self.maxpool3([X_hpool3, idx_hpool3])\n","    s3 = tf.concat([avg3, max3], axis=1)\n","    idx3 = tf.range(s3.shape[0])\n","#   Readout block\n","    s = tf.concat([s1, s2, s3], axis=0)\n","    idx = tf.concat([idx1, idx2, idx3], axis=-1)\n","    res = self.sumpool([s, idx])\n","#   MLP for classification\n","    res = self.drop(res)\n","    res = self.dense(res)\n","    \n","    return res\n","  \n","\n","class MyModel_simpleGCN(Model):\n","  def __init__(self, F, F_1, pooling_ratio, n_classes, dropout, reg):\n","    super(MyModel_simpleGCN, self).__init__()\n","    \n","    self.conv1 = Convolutional(F=F, F_1=F_1, regularizer=reg)\n","    self.hpool1 = HierarchicalPooling(F=F_1, pooling_ratio=pooling_ratio, regularizer=reg)\n","    self.avgpool1 = GlobalAvgPooling()\n","    self.maxpool1 = GlobalMaxPooling()\n","    self.conv2 = Convolutional(F=F_1, F_1=F_1, regularizer=reg)\n","    self.hpool2 = HierarchicalPooling(F=F_1, pooling_ratio=pooling_ratio, regularizer=reg)\n","    self.avgpool2 = GlobalAvgPooling()\n","    self.maxpool2 = GlobalMaxPooling()\n","    self.conv3 = Convolutional(F=F_1, F_1=F_1, regularizer=reg)\n","    self.hpool3 = HierarchicalPooling(F=F_1, pooling_ratio=pooling_ratio, regularizer=reg)\n","    self.avgpool3 = GlobalAvgPooling()\n","    self.maxpool3 = GlobalMaxPooling()\n","    self.sumpool = GlobalSumPooling()\n","    self.drop = Dropout(dropout)\n","    self.dense = Dense(n_classes, activation='softmax', kernel_regularizer=reg)\n","\n","  def call(self, inputs):\n","    X = inputs[0]\n","    A = inputs[1]\n","    idx = inputs[2]\n","    \n","#   First conv-pool block\n","    X_conv1 = self.conv1([X, A])\n","#   Second conv-pool block\n","    X_conv2 = self.conv2([X_conv1, A])\n","#   Third conv-pool block\n","    X_conv3 = self.conv3([X_conv2, A])\n","#   Readout block (GlobalMaxPooling)\n","    res = tf.math.segment_max(X_conv3, idx)\n","#   MLP for classification\n","    res = self.drop(res)\n","    res = self.dense(res)\n","    \n","#     #   First conv-pool block\n","#     X_conv1 = self.conv1([X, A])\n","#     avg1 = self.avgpool1([X_conv1, idx])\n","#     max1 = self.maxpool1([X_conv1, idx])\n","#     s1 = tf.concat([avg1, max1], axis=1)\n","#     idx1 = tf.range(s1.shape[0])\n","# #   Second conv-pool block\n","#     X_conv2 = self.conv2([X_conv1, A])\n","#     avg2 = self.avgpool2([X_conv2, idx])\n","#     max2 = self.maxpool2([X_conv2, idx])\n","#     s2 = tf.concat([avg2, max2], axis=1)\n","#     idx2 = tf.range(s2.shape[0])\n","# #   Third conv-pool block\n","#     X_conv3 = self.conv3([X_conv2, A])\n","#     avg3 = self.avgpool3([X_conv3, idx])\n","#     max3 = self.maxpool3([X_conv3, idx])\n","#     s3 = tf.concat([avg3, max3], axis=1)\n","#     idx3 = tf.range(s3.shape[0])\n","# #   Readout block (GlobalMaxPooling)\n","#     s = tf.concat([s1, s2, s3], axis=0)\n","#     idx = tf.concat([idx1, idx2, idx3], axis=-1)\n","#     res = self.sumpool([s, idx])\n","# #   MLP for classification\n","#     res = self.drop(res)\n","#     res = self.dense(res)\n","    \n","    return res"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"11NSCRia1O6L","colab_type":"code","colab":{}},"source":["### Divide dataset into train, validation and test set\n","\n","# dataset = graphs_ENZYMES\n","# dataset_name = \"ENZYMES\"\n","dataset = graphs_PROTEINS\n","dataset_name = \"PROTEINS\"\n","\n","train, val, test, n_classes = create_train_val_test_sets(dataset)\n","X_train, A_train, y_train = train\n","X_val, A_val, y_val = val\n","X_test, A_test, y_test = test\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UQzAC2TuVknB","colab_type":"code","colab":{}},"source":["### Hyperparameters\n","\n","epochs=400\n","batch_size = 64\n","\n","dropout = 0.2\n","learning_rate = 5e-3\n","momentum = 0.8\n","reg = l2(5e-3)\n","\n","F = get_numberof_features(dataset_name)\n","F_1 = 64\n","pooling_ratio = 0.8"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5BgDlJPJPG9","colab_type":"code","colab":{}},"source":["# ## Tests for bugs\n","\n","# # layer = HierarchicalPooling(F=F, pooling_ratio=pooling_ratio)\n","# # layer = GlobalAvgPooling()\n","# model = MyModel(F, F_1, pooling_ratio, n_classes, dropout, reg)\n","# generator = batch_generator([X_val, A_val, y_val], batch_size=45)\n","\n","# for X, A, y in generator:\n","#   X_batch, A_batch, idx_batch = create_batch_elements(X, A)\n","  \n","# #   X, idx = layer([X_batch, idx_batch])\n","# #   print(idx)\n","# #   X, A, idx = layer([X_batch, A_batch, idx_batch])\n","#   result = model([X_batch, A_batch, idx_batch])\n","  \n","#   print(result)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HVBJH9wElUX7","colab_type":"code","colab":{}},"source":["### Set up model and training variables\n","model = MyModel(F, F_1, pooling_ratio, n_classes, dropout, reg)\n","# model = MyModel_simpleGCN(F, F_1, pooling_ratio, n_classes, dropout, reg)\n","\n","loss_object = tf.keras.losses.CategoricalCrossentropy()\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","# optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, momentum=0)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2zHjNBQuJPLZ","colab_type":"code","colab":{}},"source":["### Training loop\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n","val_loss = tf.keras.metrics.Mean(name='val_loss')\n","val_accuracy = tf.keras.metrics.CategoricalAccuracy(name='val_accuracy')\n","\n","start_time = time.time()\n","epoch_time = start_time\n","\n","try:\n","  for epoch in range(epochs):\n","    if epoch != 0 and epoch % 100 == 0:\n","      end_time = time.time()\n","      tmp = end_time - epoch_time\n","      print(f\"Time of execution of the last 100 epochs: around {round(tmp/60)} min\")\n","      epoch_time = end_time\n","\n","    X_train, A_train, y_train = shuffle(X_train, A_train, y_train)\n","    train_generator = batch_generator([X_train, A_train, y_train], batch_size=batch_size)\n","    val_generator = batch_generator([X_val, A_val, y_val], batch_size=batch_size)\n","\n","    for X, A, y in train_generator:\n","      X_batch, A_batch, idx_batch = create_batch_elements(X, A)\n","      train_step([X_batch, A_batch, idx_batch], y)\n","\n","    for X, A, y in val_generator:\n","      X_batch, A_batch, idx_batch = create_batch_elements(X, A)\n","      val_step([X_batch, A_batch, idx_batch], y)\n","\n","    template = 'Epoch {} \\t train_loss: {:.4f}\\t train_accuracy: {:.4f}\\t val_loss: {:.4f}\\t val_accuracy: {:.4f}'\n","    print (template.format(epoch+1,\n","                           train_loss.result(),\n","                           train_accuracy.result()*100,\n","                           val_loss.result(),\n","                           val_accuracy.result()*100))\n","  \n","except KeyboardInterrupt:\n","  print(\"KeyboardInterrupt has been caught. Stopping the training...\")\n","  end_time = time.time()\n","  tmp = end_time - start_time\n","  print(f\"Time of execution of all the epochs: around {round(tmp/60)} min\")\n","  print()\n","  print()\n","  sys.exit(0)\n","\n","  \n","end_time = time.time()\n","tmp = end_time - start_time\n","print(f\"Time of execution of all the epochs: around {round(tmp/60)} min\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-rQ6ajEiJPJV","colab_type":"code","colab":{}},"source":["### Test loop\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n","val_loss = tf.keras.metrics.Mean(name='val_loss')\n","val_accuracy = tf.keras.metrics.CategoricalAccuracy(name='val_accuracy')\n","\n","test_generator = batch_generator([X_test, A_test, y_test], batch_size=batch_size)\n","val_generator = batch_generator([X_val, A_val, y_val], batch_size=batch_size)\n","\n","for X, A, y in test_generator:\n","  X_batch, A_batch, idx_batch = create_batch_elements(X, A)\n","  test_step([X_batch, A_batch, idx_batch], y)\n","  \n","for X, A, y in val_generator:\n","  X_batch, A_batch, idx_batch = create_batch_elements(X, A)\n","  val_step([X_batch, A_batch, idx_batch], y)\n","\n","template = 'test_loss: {:.4f}\\t test_accuracy: {:.4f}\\t val_loss: {:.4f}\\t val_accuracy: {:.4f}'\n","print (template.format(test_loss.result(),\n","                       test_accuracy.result()*100,\n","                       val_loss.result(),\n","                       val_accuracy.result()*100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KR92TQEEUrfw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}