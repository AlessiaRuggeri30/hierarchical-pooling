{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hierarchical pooling.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DdZPc3F7qBVm","colab_type":"text"},"source":["# Geometric Deep Learning Project - Towards Sparse Hierarchical Graph Classifiers\n","\n","*Alessia Ruggeri*\n","\n","### Implementation of the paper *Towards Sparse Hierarchical Graph Classifiers* tested on Enzymes, Proteins and D&D biological datasets using Tensorflow 2.0."]},{"cell_type":"code","metadata":{"id":"21c39ObKmnPF","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu==2.0.0-alpha0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zesQgjExrN0k","colab_type":"code","colab":{}},"source":["import os,sys,inspect\n","import networkx as nx\n","import numpy as np\n","import scipy\n","import scipy.sparse as sp\n","import matplotlib.pyplot as plt\n","import time\n","\n","import tensorflow as tf\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Layer, Input, Dense, Flatten, Activation, Dropout, ReLU\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras import Model\n","from sklearn.utils import shuffle\n","\n","from load_data import read_graphfile\n","\n","np.random.seed(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OU99Vk_Cuuw1","colab_type":"code","colab":{}},"source":["### Unzip datasets folders\n","\n","# !unzip -o data.zip\n","!unzip -o data_ENZYMES.zip\n","# !unzip -o data_PROTEINS.zip\n","# !unzip -o data_DD.zip\n","# !unzip -o data_COLLAB.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhJQXkB4rN6E","colab_type":"code","colab":{}},"source":["### Load datasets from data.zip\n","\n","# print(\"\\nLoading ENZYMES...\")\n","# graphs_ENZYMES = read_graphfile(datadir=\"data\", dataname=\"ENZYMES\", max_nodes=None)\n","\n","# print(\"\\nLoading DD...\")\n","# graphs_DD = read_graphfile(datadir=\"data\", dataname=\"DD\", max_nodes=None)\n","\n","# print(\"\\nLoading PROTEINS...\")\n","# graphs_PROTEINS = read_graphfile(datadir=\"data\", dataname=\"PROTEINS\", max_nodes=None)\n","\n","# print(\"\\nLoading COLLAB...\")\n","# graphs_COLLAB = read_graphfile(datadir=\"data\", dataname=\"COLLAB\", max_nodes=None)\n","\n","\n","### Load datasets from data_DATASET.zip\n","\n","print(\"\\nLoading ENZYMES...\")\n","graphs_ENZYMES = read_graphfile(datadir=\"data_ENZYMES\", dataname=\"ENZYMES\", max_nodes=None)\n","\n","# print(\"\\nLoading PROTEINS...\")\n","# graphs_PROTEINS = read_graphfile(datadir=\"data_PROTEINS\", dataname=\"PROTEINS\", max_nodes=None)\n","\n","# print(\"\\nLoading DD...\")\n","# graphs_DD = read_graphfile(datadir=\"data_DD\", dataname=\"DD\", max_nodes=None)\n","\n","# print(\"\\nLoading COLLAB...\")\n","# graphs_COLLAB = read_graphfile(datadir=\"data_COLLAB\", dataname=\"COLLAB\", max_nodes=None)\n","\n","print(\"\\nDone.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zAStgWLn8jVq","colab_type":"code","colab":{}},"source":["### Functions\n","\n","def get_numberof_features(dataset_name):\n","  if dataset_name == \"ENZYMES\":\n","    return 18\n","  elif dataset_name == \"PROTEINS\":\n","    return 1\n","  return None\n","\n","def preprocess_features(features):\n","  '''Row-normalize feature matrix and convert it to dense representation'''\n","  rowsum = np.array(features.sum(1))\n","  r_inv = np.power(rowsum, -1).flatten()\n","  r_inv[np.isinf(r_inv)] = 0.\n","  r_mat_inv = sp.diags(r_inv)\n","  features = r_mat_inv.dot(features)\n","  return features\n","\n","def get_adjacency_matrix(graph):\n","  '''It returns the adjacency matrix of the graph with inserted self-loops'''\n","  A = nx.adjacency_matrix(graph)\n","  A = A + np.eye(A.shape[0])\n","  A = sp.csr_matrix(A.astype(int))\n","  return A\n","  \n","def get_node_features_matrix(graph):\n","  '''It returns the node feature matrix of the graph with already preprocessed features'''\n","  Xdict = nx.get_node_attributes(graph, 'feat')\n","  X = np.array([Xdict[i] for i in range(nx.number_of_nodes(graph))])\n","  X = preprocess_features(X)\n","  X = X.astype(np.float32)\n","  return X\n","\n","def get_inverse_degree_matrix(A):\n","  '''It returns the inverse of the degree matrix constructed from the adjacency matrix of the graph'''\n","  G = nx.from_numpy_matrix(A.todense())\n","  degrees = dict(G.degree)\n","  D = np.zeros(A.shape)\n","  for i in degrees:\n","    D[i,i] = degrees[i]\n","  D = D**(-1)\n","  D = sp.csr_matrix(D.astype(int))\n","  return D\n","\n","def get_graphs_labels(dataset):\n","  '''It returns the class labels of all the graphs in the dataset'''\n","  labels = []\n","  for graph in dataset:\n","    labels.append(graph.graph['label'])\n","  labels = np.array([[labels[i]] for i in range(len(labels))])\n","  return labels\n","\n","def dot(x, y, sparse=False):\n","  '''Wrapper for tf.matmul (sparse vs dense)'''\n","  if sparse:\n","      res = tf.sparse.sparse_dense_matmul(x, y)\n","  else:\n","      res = tf.matmul(x, y)\n","  return res\n","  \n","def convert_sparse_matrix_to_sparse_tensor(coo):\n","  indices = np.transpose(np.array([coo.row, coo.col]))\n","  return tf.SparseTensor(indices, coo.data.astype(np.float32), coo.shape)\n","\n","def convert_nparray_to_sparse_tensor(nparray):\n","  tf_tensor = tf.constant(nparray)\n","  idx = tf.where(tf.not_equal(tf_tensor, 0))\n","  sparse_tensor = tf.SparseTensor(idx, tf.gather_nd(tf_tensor, idx), tf_tensor.get_shape())\n","  return sparse_tensor\n","  \n","def convert_dataset_to_arrays(dataset):\n","  feat = []\n","  adj = []\n","  diag = []\n","  for graph in dataset:\n","    X = get_node_features_matrix(graph)\n","    feat.append(X)\n","    A = get_adjacency_matrix(graph)\n","    adj.append(A.tocoo())\n","    D_inv = get_inverse_degree_matrix(A)\n","    diag.append(D_inv.tocoo())\n","#   N = total number of nodes in all the graphs\n","  X = np.concatenate(feat, axis=0)\n","#   X_batch = (N, F)\n","  A = sp.block_diag(adj)\n","  A = convert_sparse_matrix_to_sparse_tensor(A)\n","#   A = A.toarray().astype(np.float32)\n","#   A_batch = (N, N)\n","  D = sp.block_diag(diag)\n","  D = convert_sparse_matrix_to_sparse_tensor(D)\n","#   D = D.toarray().astype(np.float32)\n","#   D_batch = (N, N)  \n","  \n","  return X, A, D\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iJO_ftLm-xXB","colab_type":"code","colab":{}},"source":["### Define the layers\n","\n","class Convolutional(Layer):\n","  \n","  def __init__(self, F, F_1, **kwargs):\n","    self.F = F\n","    self.F_1 = F_1\n","    super(Convolutional, self).__init__(**kwargs)\n","\n","  def build(self, input_shape):\n","    self.W1 = self.add_weight(name='W1', \n","                             shape=(self.F, F_1),\n","                             initializer='uniform',\n","                             regularizer=None,\n","                             trainable=True)\n","\n","    self.W2 = self.add_weight(name='W2', \n","                             shape=(self.F, F_1),\n","                             initializer='uniform',\n","                             regularizer=None,\n","                             trainable=True)\n","\n","    super(Convolutional, self).build(input_shape)  # Be sure to call this at the end\n","    \n","  def call_one_graph(self, X, A, D_inv):\n","    \n","#     A = convert_sparse_matrix_to_sparse_tensor(A)\n","#     D_inv = convert_sparse_matrix_to_sparse_tensor(D_inv)\n","#     A = convert_nparray_to_sparse_tensor(A)\n","#     D_inv = convert_nparray_to_sparse_tensor(D_inv)\n","\n","\n","    res = dot(A, X, sparse=True)\n","#     res = (N, F) -> AX\n","    res = dot(D_inv, res, sparse=True)\n","#     res = (N, F) -> D**(-1)AX\n","    res = dot(res, self.W1, sparse=False)\n","#     res = (N, F_1) -> D**(-1)AXW1\n","\n","    skip_connection = dot(X, self.W2, sparse=False)\n","#     skip_connection = (N, F_1) -> XW2\n","    \n","    res = tf.math.add(res, skip_connection)\n","    res = ReLU()(res)\n","#     res = (N, F_1) -> sigma(D**(-1)AXW1 + XW2)\n","    \n","    return res\n","   \n","  def call(self, inputs):\n","    X = inputs[0]\n","    A = inputs[1]\n","    D = inputs[2]\n","\n","    res = self.call_one_graph(X, A, D)\n","    print(res)\n","\n","    return res\n","\n","  def compute_output_shape(self, input_shape):\n","    return (input_shape[0][0], self.output_dim)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"11NSCRia1O6L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"c4128cdd-cd2e-4a8e-8ea1-a72ddd8f3b6d","executionInfo":{"status":"ok","timestamp":1557331636910,"user_tz":-120,"elapsed":2107,"user":{"displayName":"Alessia Ruggeri","photoUrl":"","userId":"01098494933204128714"}}},"source":["dataset = graphs_ENZYMES\n","dataset_name = \"ENZYMES\"\n","# dataset = graphs_PROTEINS\n","# dataset_name = \"PROTEINS\"\n","\n","labels = get_graphs_labels(dataset)\n","\n","dataset, labels = shuffle(dataset, labels)\n","\n","train_n = len(dataset)//100 * 80\n","val_n = len(dataset)//100 * 15\n","\n","(x_val, y_val) = dataset[0:val_n], labels[0:val_n]\n","(x_train, y_train) = dataset[val_n:train_n], labels[val_n:train_n]\n","(x_test, y_test) = dataset[train_n:], labels[train_n:]\n","\n","print('train:', len(x_train), len(y_train))\n","print('val:', len(x_val), len(y_val))\n","print('test:', len(x_test), len(y_test))\n","\n","X_train, A_train, D_train = convert_dataset_to_arrays(x_train)\n","X_val, A_val, D_val = convert_dataset_to_arrays(x_val)\n","X_test, A_test, D_test = convert_dataset_to_arrays(x_test)"],"execution_count":296,"outputs":[{"output_type":"stream","text":["train: 390 390\n","val: 90 90\n","test: 120 120\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: RuntimeWarning: divide by zero encountered in reciprocal\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"UQzAC2TuVknB","colab_type":"code","colab":{}},"source":["# Hyperparameters\n","\n","epochs=10\n","batch_size = 32\n","n_classes=len(np.unique(labels))\n","\n","dropout = 0.5\n","learning_rate = 5e-4\n","reg = l2(5e-4)\n","momentum = 0.8\n","\n","F = get_numberof_features(dataset_name)\n","F_1 = 128"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5BgDlJPJPG9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":255},"outputId":"3f5d60be-cc6c-4f35-effe-f591dae895f4","executionInfo":{"status":"ok","timestamp":1557331643401,"user_tz":-120,"elapsed":1341,"user":{"displayName":"Alessia Ruggeri","photoUrl":"","userId":"01098494933204128714"}}},"source":["layer = Convolutional(F=F, F_1=F_1, input_shape=(F,))\n","result = layer([X_val, A_val, D_val])"],"execution_count":298,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[9.5753224e+18 0.0000000e+00 2.4092933e+18 ... 9.4477945e+18\n","  7.0990480e+18 5.8876781e+18]\n"," [9.6580980e+18 0.0000000e+00 2.4024359e+18 ... 9.5481777e+18\n","  7.1600621e+18 5.9334920e+18]\n"," [9.5739502e+18 0.0000000e+00 2.3790271e+18 ... 9.4629743e+18\n","  7.0853502e+18 5.8929392e+18]\n"," ...\n"," [1.3025328e+19 0.0000000e+00 1.7958751e+18 ... 1.4069941e+19\n","  9.2075836e+18 9.3753070e+18]\n"," [1.3097568e+19 0.0000000e+00 1.8128007e+18 ... 1.4148972e+19\n","  9.2587114e+18 9.4277921e+18]\n"," [1.3098026e+19 0.0000000e+00 1.8618418e+18 ... 1.4129348e+19\n","  9.2701716e+18 9.4040713e+18]], shape=(3023, 128), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vowiaci6FQWP","colab_type":"code","colab":{}},"source":["# X_in = Input(shape=(F,))\n","# A_in = Input()\n","\n","# conv1 = Convolutional(F=F, F_1=F_1)([X_in, A_in])\n","# conv2 = Convolutional(F=F_1, F_1=F_1)([conv1, A_in])\n","# conv3 = Convolutional(F=F_1, F_1=F_1)([conv2, A_in])\n","# flat = Flatten()(conv3)\n","# dense1 = Dense(256, activation='relu', kernel_regularizer=reg)(flat)\n","# drop = Dropout(dropout)(dense1)\n","# dense2 = Dense(n_classes, activation='softmax', kernel_regularizer=reg)(drop)\n","\n","# model = Model(inputs=[X_in, A_in], outputs=dense2)\n","\n","model = Sequential()\n","model.add(Convolutional(F=F, F_1=F_1))\n","model.add(Convolutional(F=F_1, F_1=F_1))\n","model.add(Convolutional(F=F_1, F_1=F_1))\n","model.add(Flatten())\n","model.add(Dense(256, activation='relu', kernel_regularizer=reg))\n","model.add(Dropout(dropout))\n","model.add(Dense(n_classes, activation='softmax', kernel_regularizer=reg))\n","\n","\n","model.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-rQ6ajEiJPJV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":307},"outputId":"57e8c8f1-165e-45b0-e14d-bde446af8c84","executionInfo":{"status":"error","timestamp":1557331662843,"user_tz":-120,"elapsed":921,"user":{"displayName":"Alessia Ruggeri","photoUrl":"","userId":"01098494933204128714"}}},"source":["model.fit([X_train, A_train, D_train], y_train, validation_data=([X_val, A_val, D_val], y_val), epochs=epochs, batch_size=batch_size)"],"execution_count":301,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-301-2231883bec2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2501\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2502\u001b[0m         \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2503\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2504\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2505\u001b[0m       \u001b[0my_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m   2741\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2742\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2743\u001b[0;31m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2744\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"]}]},{"cell_type":"code","metadata":{"id":"2zHjNBQuJPLZ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}