{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hierarchical pooling.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DdZPc3F7qBVm","colab_type":"text"},"source":["# Geometric Deep Learning Project - Towards Sparse Hierarchical Graph Classifiers\n","\n","*Alessia Ruggeri*\n","\n","### Implementation of the paper *Towards Sparse Hierarchical Graph Classifiers* tested on Enzymes, Proteins and D&D biological datasets using Tensorflow 2.0."]},{"cell_type":"code","metadata":{"id":"21c39ObKmnPF","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu==2.0.0-alpha0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zesQgjExrN0k","colab_type":"code","colab":{}},"source":["import os,sys,inspect\n","import networkx as nx\n","import numpy as np\n","import scipy\n","import scipy.sparse as sp\n","import matplotlib.pyplot as plt\n","import time\n","import math\n","\n","import tensorflow as tf\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Layer, Input, Dense, Flatten, Activation, Dropout, ReLU\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras import Model\n","from sklearn.utils import shuffle\n","\n","from load_data import read_graphfile\n","\n","np.random.seed(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OU99Vk_Cuuw1","colab_type":"code","colab":{}},"source":["### Unzip datasets folders\n","\n","# !unzip -o data.zip\n","!unzip -o data_ENZYMES.zip\n","# !unzip -o data_PROTEINS.zip\n","# !unzip -o data_DD.zip\n","# !unzip -o data_COLLAB.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhJQXkB4rN6E","colab_type":"code","colab":{}},"source":["### Load datasets from data.zip\n","\n","# print(\"\\nLoading ENZYMES...\")\n","# graphs_ENZYMES = read_graphfile(datadir=\"data\", dataname=\"ENZYMES\", max_nodes=None)\n","\n","# print(\"\\nLoading DD...\")\n","# graphs_DD = read_graphfile(datadir=\"data\", dataname=\"DD\", max_nodes=None)\n","\n","# print(\"\\nLoading PROTEINS...\")\n","# graphs_PROTEINS = read_graphfile(datadir=\"data\", dataname=\"PROTEINS\", max_nodes=None)\n","\n","# print(\"\\nLoading COLLAB...\")\n","# graphs_COLLAB = read_graphfile(datadir=\"data\", dataname=\"COLLAB\", max_nodes=None)\n","\n","\n","### Load datasets from data_DATASET.zip\n","\n","print(\"\\nLoading ENZYMES...\")\n","graphs_ENZYMES = read_graphfile(datadir=\"data_ENZYMES\", dataname=\"ENZYMES\", max_nodes=None)\n","\n","# print(\"\\nLoading PROTEINS...\")\n","# graphs_PROTEINS = read_graphfile(datadir=\"data_PROTEINS\", dataname=\"PROTEINS\", max_nodes=None)\n","\n","# print(\"\\nLoading DD...\")\n","# graphs_DD = read_graphfile(datadir=\"data_DD\", dataname=\"DD\", max_nodes=None)\n","\n","# print(\"\\nLoading COLLAB...\")\n","# graphs_COLLAB = read_graphfile(datadir=\"data_COLLAB\", dataname=\"COLLAB\", max_nodes=None)\n","\n","print(\"\\nDone.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o6SUikFsKB2x","colab_type":"code","colab":{}},"source":["### Generic functions\n","\n","def get_numberof_features(dataset_name):\n","  if dataset_name == \"ENZYMES\":\n","    return 18\n","  elif dataset_name == \"PROTEINS\":\n","    return 1\n","  return None\n","\n","def preprocess_features(features):\n","  '''Row-normalize feature matrix and convert it to dense representation'''\n","  rowsum = np.array(features.sum(1))\n","  r_inv = np.power(rowsum, -1).flatten()\n","  r_inv[np.isinf(r_inv)] = 0.\n","  r_mat_inv = sp.diags(r_inv)\n","  features = r_mat_inv.dot(features)\n","  return features\n","\n","def get_node_features_matrix(graph):\n","  '''It returns the node feature matrix of the graph with already preprocessed features'''\n","  Xdict = nx.get_node_attributes(graph, 'feat')\n","  X = np.array([Xdict[i] for i in range(nx.number_of_nodes(graph))])\n","  X = preprocess_features(X)\n","  X = X.astype(np.float32)\n","  return X\n","\n","def get_adjacency_matrix(graph):\n","  '''It returns the adjacency matrix of the graph with inserted self-loops'''\n","  A = nx.adjacency_matrix(graph)\n","  A = np.array(A + np.eye(A.shape[0]))\n","  A = sp.coo_matrix(A.astype(np.float32))\n","  return A\n","\n","def get_normalization_matrix(A):\n","  '''It returns the normalized adjacency matrix of the graph'''\n","  degrees = np.array(np.sum(A.todense(), axis=1)).flatten()\n","  degrees = np.power(degrees, -1)\n","  degrees[np.isinf(degrees)] = 0\n","  degrees = degrees.astype(np.float32)\n","  D = sp.diags(degrees, offsets=0).tocoo()\n","  return D\n","\n","def get_normalized_adjacency_matrix(A):\n","  D = get_normalization_matrix(A)\n","  A_norm = D @ A\n","  return A_norm.tocoo()\n","\n","def get_graphs_labels(dataset):\n","  '''It returns the class labels of all the graphs in the dataset'''\n","  labels = []\n","  for graph in dataset:\n","    labels.append(graph.graph['label'])\n","  labels = np.array([[labels[i]] for i in range(len(labels))])\n","  return labels\n","\n","def dot(x, y, sparse=False):\n","  '''Wrapper for tf.matmul (sparse vs dense)'''\n","  if sparse:\n","      res = tf.sparse.sparse_dense_matmul(x, y)\n","  else:\n","      res = tf.matmul(x, y)\n","  return res\n","  \n","def convert_sparse_matrix_to_sparse_tensor(coo):\n","  indices = np.transpose(np.array([coo.row, coo.col]))\n","  return tf.SparseTensor(indices, coo.data.astype(np.float32), coo.shape)\n","\n","def convert_nparray_to_sparse_tensor(nparray):\n","  tf_tensor = tf.constant(nparray)\n","  idx = tf.where(tf.not_equal(tf_tensor, 0))\n","  sparse_tensor = tf.SparseTensor(idx, tf.gather_nd(tf_tensor, idx), tf_tensor.get_shape())\n","  return sparse_tensor\n","\n","def convert_tf_tensor_to_sparse_tensor(tf_tensor):\n","  idx = tf.where(tf.not_equal(tf_tensor, 0))\n","  sparse_tensor = tf.SparseTensor(idx, tf.gather_nd(tf_tensor, idx), tf_tensor.get_shape())\n","  return sparse_tensor\n","\n","def one_hot_encoding(data, n_classes):\n","    '''It one-hot encode data'''\n","    targets = np.array(data).reshape(-1)\n","    targets = np.eye(n_classes)[targets]\n","    return targets\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"paqyqK-Zf_j_","colab_type":"code","colab":{}},"source":["### Execution functions\n","\n","def convert_dataset_to_lists(dataset):\n","  feat = []\n","  adj = []\n","  for graph in dataset:\n","    X = get_node_features_matrix(graph)\n","    A = get_adjacency_matrix(graph)\n","    A_norm = get_normalized_adjacency_matrix(A)\n","    feat.append(X)\n","    adj.append(A_norm)\n","  return feat, adj\n","\n","def create_batch_elements(X, A):\n","  '''It takes X and A lists and creates respective stack of nodes, block diagonal adjacency matrix and graph idx array'''\n","  X_stack = np.vstack(X)\n","  A_diag = sp.block_diag(A)\n","  A_diag = convert_sparse_matrix_to_sparse_tensor(A_diag)\n","  A_diag = tf.sparse.reorder(A_diag)\n","  n_nodes = np.array([a.shape[0] for a in A])\n","  graph_idx = np.repeat(np.arange(len(n_nodes)), n_nodes)\n","  return X_stack, A_diag, graph_idx\n","\n","def batch_generator(data, batch_size=32):\n","  '''It takes a list of arrays or matrices and it yields batches of given size'''\n","  len_data = len(data[0])\n","  batches_per_epoch = math.ceil(len_data/batch_size)\n","  for batch in range(batches_per_epoch):\n","    start = batch * batch_size\n","    end = min(start+batch_size, len_data)\n","    out = [item[start:end] for item in data]\n","    yield out\n","\n","# @tf.function\n","def train_step(data, labels):\n","  with tf.GradientTape() as tape:\n","    predictions = model(data)\n","    loss = loss_object(labels, predictions)\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","  train_loss(loss)\n","  train_accuracy(labels, predictions)\n","  \n","# @tf.function\n","def val_step(data, labels):\n","  predictions = model(data)\n","  t_loss = loss_object(labels, predictions)\n","\n","  val_loss(t_loss)\n","  val_accuracy(labels, predictions)\n","  \n","# @tf.function\n","def test_step(data, labels):\n","  predictions = model(data)\n","  t_loss = loss_object(labels, predictions)\n","\n","  test_loss(t_loss)\n","  test_accuracy(labels, predictions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZTbbrtxXKSe","colab_type":"code","colab":{}},"source":["### Define the layers\n","\n","\n","class Convolutional(Layer):\n","  \n","  def __init__(self, F, F_1, **kwargs):\n","    self.F = F\n","    self.F_1 = F_1\n","    super(Convolutional, self).__init__(**kwargs)\n","\n","  def build(self, input_shape):\n","    self.W1 = self.add_weight(name='W1', \n","                             shape=(self.F, F_1),\n","                             initializer='uniform',\n","                             regularizer=None,\n","                             trainable=True)\n","\n","    self.W2 = self.add_weight(name='W2', \n","                             shape=(self.F, F_1),\n","                             initializer='uniform',\n","                             regularizer=None,\n","                             trainable=True)\n","\n","    super(Convolutional, self).build(input_shape)  # Be sure to call this at the end\n","    \n","  def kernel(self, X, A):\n","    \n","    res = dot(A, X, sparse=True)\n","#     res = (N, F) -> AX\n","    res = dot(res, self.W1, sparse=False)\n","#     res = (N, F_1) -> AXW1\n","    skip_connection = dot(X, self.W2, sparse=False)\n","#     skip_connection = (N, F_1) -> XW2\n","    \n","    res = tf.math.add(res, skip_connection)\n","    res = ReLU()(res)\n","#     res = (N, F_1) -> sigma(AXW1 + XW2)\n","    \n","    return res\n","   \n","  def call(self, inputs):\n","    X = inputs[0]\n","    A = inputs[1]\n","\n","    res = self.kernel(X, A)\n","\n","    return res\n","\n","  def compute_output_shape(self, input_shape):\n","    return (input_shape[0][0], self.F_1)\n","\n","  \n","class GlobalAvgPooling(Layer):\n","  \n","  def __init__(self, **kwargs):\n","    super(GlobalAvgPooling, self).__init__(**kwargs)\n","    \n","  def build(self, input_shape):\n","    super(GlobalAvgPooling, self).build(input_shape)  # Be sure to call this at the end\n","  \n","  def call(self, inputs):\n","    nodes_feat = inputs[0]\n","    idx = inputs[1]\n","    \n","    res = tf.math.segment_mean(nodes_feat, idx)\n","    idx = tf.range(res.shape[0])\n","    \n","    return res, idx\n","  \n","  def compute_output_shape(self, input_shape):\n","    return (len(np.unique(idx)), self.F_1)\n","  \n","  \n","class GlobalSumPooling(Layer):\n","  \n","  def __init__(self, **kwargs):\n","    super(GlobalSumPooling, self).__init__(**kwargs)\n","    \n","  def build(self, input_shape):\n","    super(GlobalSumPooling, self).build(input_shape)  # Be sure to call this at the end\n","  \n","  def call(self, inputs):\n","    nodes_feat = inputs[0]\n","    idx = inputs[1]\n","    \n","    res = tf.math.segment_sum(nodes_feat, idx)\n","    \n","    return res\n","  \n","  def compute_output_shape(self, input_shape):\n","    return (len(np.unique(idx)), self.F_1)\n","  \n","  \n","class HierarchicalPooling(Layer):\n","  \n","  def __init__(self, F, pooling_ratio, **kwargs):\n","    self.F = F\n","    self.k = pooling_ratio\n","    super(HierarchicalPooling, self).__init__(**kwargs)\n","  \n","  def build(self, input_shape):\n","    \n","    self.p = self.add_weight(name='p', \n","                             shape=(self.F, 1),\n","                             initializer='uniform',\n","                             regularizer=None,\n","                             trainable=True)\n","    \n","    super(HierarchicalPooling, self).build(input_shape)  # Be sure to call this at the end\n","  \n","  def call(self, inputs):\n","    X = inputs[0]\n","    A = inputs[1]\n","    idx = inputs[2]\n","    A = tf.sparse.to_dense(A)\n","    \n","#   Generate projection scores\n","    p_norm = tf.math.l2_normalize(self.p, axis=0)\n","    proj = tf.math.divide(self.p, p_norm)\n","    y = dot(X, proj, sparse=False)\n","#     y = (N, 1)\n","    \n","    feat = []\n","    adj = []\n","    indices = []\n","    for g in range(tf.size(tf.unique(idx)[0])):\n","#     Select only nodes of one graph\n","      gg = tf.reshape(tf.where(tf.equal(idx, g)), [-1])\n","      X_g = tf.gather(X, indices=gg)\n","      A_g = tf.gather(tf.gather(A, gg, axis=0),  gg, axis=1)\n","      idx_g = tf.gather(idx, gg, axis=0)\n","      y_g = tf.gather(y, gg, axis=0)\n","      \n","#     Generate indices of nodes to keep for that graph\n","      N = X_g.shape[0]\n","      to_keep = tf.cast(tf.math.ceil(N*self.k), dtype=tf.int32)\n","      i = tf.argsort(y_g, axis=0, direction='DESCENDING')\n","      i = tf.reshape(i, [-1])[:to_keep]\n","      \n","#     Filter X, A and idx of the graph using i to keep only nodes with higher projection score\n","      X_g = tf.gather(X_g, indices=i)\n","      A_g = tf.gather(tf.gather(A_g, i, axis=0),  i, axis=1)\n","      idx_g = tf.gather(idx_g, i, axis=0)\n","      \n","      y_tilde = tf.tanh(tf.gather(y_g, i, axis=0))\n","      X_tilde = tf.multiply(X_g, y_tilde)\n","      \n","      feat.append(X_tilde)\n","      adj.append(A_g)\n","      indices.append(idx_g)\n","    \n","#   Rebuild X, A and idx for the entire batch\n","    X_stack = tf.concat(feat, axis=0)\n","    A_diag = sp.block_diag(adj)\n","    A_diag = convert_sparse_matrix_to_sparse_tensor(A_diag)\n","    A_diag = tf.sparse.reorder(A_diag)\n","    idx_seq = tf.concat(indices, axis=-1)\n","    \n","    return X_stack, A_diag, idx_seq\n","\n","\n","class MyModel(Model):\n","  def __init__(self, F, F_1, pooling_ratio, n_classes, dropout, reg):\n","    super(MyModel, self).__init__()\n","    \n","    self.conv1 = Convolutional(F=F, F_1=F_1)\n","    self.avgpool1 = GlobalAvgPooling()\n","    self.hpool1 = HierarchicalPooling(F=F_1, pooling_ratio=pooling_ratio)\n","    self.conv2 = Convolutional(F=F_1, F_1=F_1)\n","    self.avgpool2 = GlobalAvgPooling()\n","    self.hpool2 = HierarchicalPooling(F=F_1, pooling_ratio=pooling_ratio)\n","    self.conv3 = Convolutional(F=F_1, F_1=F_1)\n","    self.avgpool3 = GlobalAvgPooling()\n","    self.hpool3 = HierarchicalPooling(F=F_1, pooling_ratio=pooling_ratio)\n","    self.sumpool = GlobalSumPooling()\n","    self.flat = Flatten()\n","    self.dense1 = Dense(256, activation='relu', kernel_regularizer=reg)\n","    self.drop = Dropout(dropout)\n","    self.dense2 = Dense(n_classes, activation='softmax', kernel_regularizer=reg)\n","\n","  def call(self, inputs):\n","    X = inputs[0]\n","    A = inputs[1]\n","    idx = inputs[2]\n","    \n","#   First conv-pool block\n","    X_conv1 = self.conv1([X, A])\n","    avg1, idx_avgpool1 = self.avgpool1([X_conv1, idx])\n","    X_hpool1, A_hpool1, idx_hpool1 = self.hpool1([X_conv1, A, idx])\n","    s1 = tf.concat([avg1, X_hpool1], axis=0)\n","    idx1 = tf.concat([idx_avgpool1, tf.cast(idx_hpool1, dtype=tf.int32)], axis=-1)\n","#   Second conv-pool block\n","    X_conv2 = self.conv2([X_hpool1, A_hpool1])\n","    avg2, idx_avgpool2 = self.avgpool2([X_conv2, idx_hpool1])\n","    X_hpool2, A_hpool2, idx_hpool2 = self.hpool2([X_conv2, A_hpool1, idx_hpool1])\n","    s2 = tf.concat([avg2, X_hpool2], axis=0)\n","    idx2 = tf.concat([idx_avgpool2, tf.cast(idx_hpool2, dtype=tf.int32)], axis=-1)\n","#   Third conv-pool block\n","    X_conv3 = self.conv3([X_hpool2, A_hpool2])\n","    avg3, idx_avgpool3 = self.avgpool3([X_conv3, idx_hpool2])\n","    X_hpool3, A_hpool3, idx_hpool3 = self.hpool3([X_conv3, A_hpool2, idx_hpool2])\n","    s3 = tf.concat([avg3, X_hpool3], axis=0)\n","    idx3 = tf.concat([idx_avgpool3, tf.cast(idx_hpool3, dtype=tf.int32)], axis=-1)\n","#   Readout block\n","    s = tf.concat([s1, s2, s3], axis=0)\n","    idx = tf.concat([idx1, idx2, idx3], axis=-1)\n","    res = self.sumpool([s, idx])\n","    res = self.flat(res)\n","#   MLP for classification\n","#     res = self.dense1(res)\n","#     res = self.drop(res)\n","    res = self.dense2(res)\n","    \n","    return res"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"11NSCRia1O6L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"73c3404b-7829-4331-ca5a-658cf1953c40","executionInfo":{"status":"ok","timestamp":1557510010394,"user_tz":-120,"elapsed":1515,"user":{"displayName":"Alessia Ruggeri","photoUrl":"","userId":"01098494933204128714"}}},"source":["dataset = graphs_ENZYMES\n","dataset_name = \"ENZYMES\"\n","# dataset = graphs_PROTEINS\n","# dataset_name = \"PROTEINS\"\n","\n","labels = get_graphs_labels(dataset)\n","n_classes=len(np.unique(labels))\n","\n","dataset, labels = shuffle(dataset, labels)\n","\n","train_n = len(dataset)//100 * 80\n","val_n = len(dataset)//100 * 15\n","\n","(x_val, y_val) = dataset[0:val_n], labels[0:val_n]\n","(x_train, y_train) = dataset[val_n:train_n], labels[val_n:train_n]\n","(x_test, y_test) = dataset[train_n:], labels[train_n:]\n","\n","print('train:', len(x_train))\n","print('val:', len(x_val))\n","print('test:', len(x_test))\n","\n","X_train, A_train = convert_dataset_to_lists(x_train)\n","X_val, A_val = convert_dataset_to_lists(x_val)\n","X_test, A_test = convert_dataset_to_lists(x_test)\n","\n","y_train = one_hot_encoding(y_train, n_classes)\n","y_val = one_hot_encoding(y_val, n_classes)\n","y_test = one_hot_encoding(y_test, n_classes)\n"],"execution_count":356,"outputs":[{"output_type":"stream","text":["train: 390\n","val: 90\n","test: 120\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UQzAC2TuVknB","colab_type":"code","colab":{}},"source":["# Hyperparameters\n","\n","epochs=5000\n","batch_size = 32\n","\n","dropout = 0.5\n","learning_rate = 5e-3\n","reg = l2(5e-4)\n","\n","F = get_numberof_features(dataset_name)\n","F_1 = 128\n","pooling_ratio = 0.8"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5BgDlJPJPG9","colab_type":"code","colab":{}},"source":["# layer = HierarchicalPooling(F=F, pooling_ratio=pooling_ratio)\n","# layer = GlobalAvgPooling()\n","model = MyModel(F, F_1, pooling_ratio, n_classes, dropout, reg)\n","generator = batch_generator([X_val, A_val, y_val], batch_size=45)\n","\n","for X, A, y in generator:\n","  X_batch, A_batch, idx_batch = create_batch_elements(X, A)\n","  \n","#   X, idx = layer([X_batch, idx_batch])\n","#   print(idx)\n","#   X, A, idx = layer([X_batch, A_batch, idx_batch])\n","  result = model([X_batch, A_batch, idx_batch])\n","  \n","#   print(result)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HVBJH9wElUX7","colab_type":"code","colab":{}},"source":["### Set up model and training variables\n","model = MyModel(F, F_1, pooling_ratio, n_classes, dropout, reg)\n","\n","loss_object = tf.keras.losses.CategoricalCrossentropy()\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2zHjNBQuJPLZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1717},"outputId":"42cad7d4-54ef-4caf-b933-1ce823edb370","executionInfo":{"status":"ok","timestamp":1557509923861,"user_tz":-120,"elapsed":1063304,"user":{"displayName":"Alessia Ruggeri","photoUrl":"","userId":"01098494933204128714"}}},"source":["### Training loop\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n","val_loss = tf.keras.metrics.Mean(name='val_loss')\n","val_accuracy = tf.keras.metrics.CategoricalAccuracy(name='val_accuracy')\n","\n","start_time = time.time()\n","epoch_time = start_time\n","\n","for epoch in range(epochs):\n","  if epoch != 0 and epoch % 100 == 0:\n","    end_time = time.time()\n","    tmp = end_time - epoch_time\n","    print(f\"Time of execution of the last 100 epochs: around {round(tmp/60)} min\")\n","    epoch_time = end_time\n","  \n","  X_train, A_train, y_train = shuffle(X_train, A_train, y_train)\n","  train_generator = batch_generator([X_train, A_train, y_train], batch_size=batch_size)\n","  val_generator = batch_generator([X_val, A_val, y_val], batch_size=batch_size)\n","  \n","  for X, A, y in train_generator:\n","    X_batch, A_batch, idx_batch = create_batch_elements(X, A)\n","    train_step([X_batch, A_batch, idx_batch], y)\n","\n","  for X, A, y in val_generator:\n","    X_batch, A_batch, idx_batch = create_batch_elements(X, A)\n","    val_step([X_batch, A_batch, idx_batch], y)\n","\n","  template = 'Epoch {} \\t train_loss: {:.4f}\\t train_accuracy: {:.4f}\\t val_loss: {:.4f}\\t val_accuracy: {:.4f}'\n","  print (template.format(epoch+1,\n","                         train_loss.result(),\n","                         train_accuracy.result()*100,\n","                         val_loss.result(),\n","                         val_accuracy.result()*100))\n","\n","  \n","end_time = time.time()\n","tmp = end_time - start_time\n","print(f\"Time of execution of all the epochs: around {round(tmp/60)} min\")"],"execution_count":352,"outputs":[{"output_type":"stream","text":["Epoch 1 \t train_loss: 1.7740\t train_accuracy: 19.2308\t val_loss: 1.8184\t val_accuracy: 12.2222\n","Epoch 2 \t train_loss: 1.7718\t train_accuracy: 22.0513\t val_loss: 1.8154\t val_accuracy: 12.2222\n","Epoch 3 \t train_loss: 1.7641\t train_accuracy: 22.7350\t val_loss: 1.8654\t val_accuracy: 12.2222\n","Epoch 4 \t train_loss: 1.7664\t train_accuracy: 23.1410\t val_loss: 1.9646\t val_accuracy: 11.6667\n","Epoch 5 \t train_loss: 1.7758\t train_accuracy: 23.2308\t val_loss: 1.9920\t val_accuracy: 13.3333\n","Epoch 6 \t train_loss: 1.7784\t train_accuracy: 23.2479\t val_loss: 1.9950\t val_accuracy: 12.5926\n","Epoch 7 \t train_loss: 1.7766\t train_accuracy: 22.8205\t val_loss: 2.0118\t val_accuracy: 14.1270\n","Epoch 8 \t train_loss: 1.7851\t train_accuracy: 23.0128\t val_loss: 2.0437\t val_accuracy: 14.5833\n","Epoch 9 \t train_loss: 1.7885\t train_accuracy: 22.5641\t val_loss: 2.0373\t val_accuracy: 14.9383\n","Epoch 10 \t train_loss: 1.7874\t train_accuracy: 22.7436\t val_loss: 2.0392\t val_accuracy: 14.6667\n","Epoch 11 \t train_loss: 1.7863\t train_accuracy: 22.9138\t val_loss: 2.0302\t val_accuracy: 15.7576\n","Epoch 12 \t train_loss: 1.7803\t train_accuracy: 23.0983\t val_loss: 2.0300\t val_accuracy: 16.3889\n","Epoch 13 \t train_loss: 1.7839\t train_accuracy: 22.9389\t val_loss: 2.0235\t val_accuracy: 16.8376\n","Epoch 14 \t train_loss: 1.7854\t train_accuracy: 23.1502\t val_loss: 2.0115\t val_accuracy: 17.3016\n","Epoch 15 \t train_loss: 1.7835\t train_accuracy: 23.0427\t val_loss: 1.9962\t val_accuracy: 17.6296\n","Epoch 16 \t train_loss: 1.7812\t train_accuracy: 23.0288\t val_loss: 1.9855\t val_accuracy: 17.5000\n","Epoch 17 \t train_loss: 1.7766\t train_accuracy: 23.1825\t val_loss: 1.9792\t val_accuracy: 17.9085\n","Epoch 18 \t train_loss: 1.7731\t train_accuracy: 23.1766\t val_loss: 1.9795\t val_accuracy: 18.4568\n","Epoch 19 \t train_loss: 1.7719\t train_accuracy: 23.3063\t val_loss: 1.9756\t val_accuracy: 18.9474\n","Epoch 20 \t train_loss: 1.7693\t train_accuracy: 23.4487\t val_loss: 1.9738\t val_accuracy: 19.4444\n","Epoch 21 \t train_loss: 1.7705\t train_accuracy: 23.4310\t val_loss: 1.9706\t val_accuracy: 20.0000\n","Epoch 22 \t train_loss: 1.7719\t train_accuracy: 23.6247\t val_loss: 1.9698\t val_accuracy: 19.8990\n","Epoch 23 \t train_loss: 1.7705\t train_accuracy: 23.6343\t val_loss: 1.9694\t val_accuracy: 19.6618\n","Epoch 24 \t train_loss: 1.7689\t train_accuracy: 23.6004\t val_loss: 1.9682\t val_accuracy: 19.7222\n","Epoch 25 \t train_loss: 1.7694\t train_accuracy: 23.4667\t val_loss: 1.9668\t val_accuracy: 20.0889\n","Epoch 26 \t train_loss: 1.7688\t train_accuracy: 23.5108\t val_loss: 1.9625\t val_accuracy: 20.3419\n","Epoch 27 \t train_loss: 1.7702\t train_accuracy: 23.4473\t val_loss: 1.9625\t val_accuracy: 20.2469\n","Epoch 28 \t train_loss: 1.7682\t train_accuracy: 23.4799\t val_loss: 1.9591\t val_accuracy: 20.3175\n","Epoch 29 \t train_loss: 1.7676\t train_accuracy: 23.5367\t val_loss: 1.9528\t val_accuracy: 20.6897\n","Epoch 30 \t train_loss: 1.7659\t train_accuracy: 23.5299\t val_loss: 1.9471\t val_accuracy: 20.7778\n","Epoch 31 \t train_loss: 1.7642\t train_accuracy: 23.5815\t val_loss: 1.9418\t val_accuracy: 21.0036\n","Epoch 32 \t train_loss: 1.7622\t train_accuracy: 23.7260\t val_loss: 1.9384\t val_accuracy: 21.2500\n","Epoch 33 \t train_loss: 1.7610\t train_accuracy: 23.7918\t val_loss: 1.9362\t val_accuracy: 21.5825\n","Epoch 34 \t train_loss: 1.7589\t train_accuracy: 23.9367\t val_loss: 1.9351\t val_accuracy: 21.7320\n","Epoch 35 \t train_loss: 1.7576\t train_accuracy: 24.0220\t val_loss: 1.9401\t val_accuracy: 21.8413\n","Epoch 36 \t train_loss: 1.7607\t train_accuracy: 24.0456\t val_loss: 1.9415\t val_accuracy: 22.2222\n","Epoch 37 \t train_loss: 1.7630\t train_accuracy: 23.9570\t val_loss: 1.9404\t val_accuracy: 22.4324\n","Epoch 38 \t train_loss: 1.7617\t train_accuracy: 23.9069\t val_loss: 1.9363\t val_accuracy: 22.6608\n","Epoch 39 \t train_loss: 1.7606\t train_accuracy: 24.0039\t val_loss: 1.9335\t val_accuracy: 22.9060\n","Epoch 40 \t train_loss: 1.7603\t train_accuracy: 24.0577\t val_loss: 1.9313\t val_accuracy: 23.0278\n","Epoch 41 \t train_loss: 1.7586\t train_accuracy: 24.0525\t val_loss: 1.9303\t val_accuracy: 23.2249\n","Epoch 42 \t train_loss: 1.7572\t train_accuracy: 24.1392\t val_loss: 1.9293\t val_accuracy: 23.3598\n","Epoch 43 \t train_loss: 1.7565\t train_accuracy: 24.1562\t val_loss: 1.9263\t val_accuracy: 23.5401\n","Epoch 44 \t train_loss: 1.7547\t train_accuracy: 24.2075\t val_loss: 1.9262\t val_accuracy: 23.4848\n","Epoch 45 \t train_loss: 1.7534\t train_accuracy: 24.2564\t val_loss: 1.9235\t val_accuracy: 23.6296\n","Epoch 46 \t train_loss: 1.7521\t train_accuracy: 24.3255\t val_loss: 1.9227\t val_accuracy: 23.5507\n","Epoch 47 \t train_loss: 1.7503\t train_accuracy: 24.4244\t val_loss: 1.9209\t val_accuracy: 23.5934\n","Epoch 48 \t train_loss: 1.7500\t train_accuracy: 24.5032\t val_loss: 1.9195\t val_accuracy: 23.6111\n","Epoch 49 \t train_loss: 1.7500\t train_accuracy: 24.5840\t val_loss: 1.9163\t val_accuracy: 23.7415\n","Epoch 50 \t train_loss: 1.7494\t train_accuracy: 24.6667\t val_loss: 1.9145\t val_accuracy: 23.9778\n","Epoch 51 \t train_loss: 1.7488\t train_accuracy: 24.6757\t val_loss: 1.9134\t val_accuracy: 24.0959\n","Epoch 52 \t train_loss: 1.7478\t train_accuracy: 24.7239\t val_loss: 1.9118\t val_accuracy: 24.1453\n","Epoch 53 \t train_loss: 1.7464\t train_accuracy: 24.7121\t val_loss: 1.9105\t val_accuracy: 24.1509\n","Epoch 54 \t train_loss: 1.7459\t train_accuracy: 24.7483\t val_loss: 1.9097\t val_accuracy: 24.3210\n","Epoch 55 \t train_loss: 1.7450\t train_accuracy: 24.7646\t val_loss: 1.9074\t val_accuracy: 24.3838\n","Epoch 56 \t train_loss: 1.7449\t train_accuracy: 24.8031\t val_loss: 1.9057\t val_accuracy: 24.4444\n","Epoch 57 \t train_loss: 1.7451\t train_accuracy: 24.8493\t val_loss: 1.9081\t val_accuracy: 24.2885\n","Epoch 58 \t train_loss: 1.7446\t train_accuracy: 24.8939\t val_loss: 1.9099\t val_accuracy: 24.3870\n","Epoch 59 \t train_loss: 1.7437\t train_accuracy: 24.9239\t val_loss: 1.9101\t val_accuracy: 24.2561\n","Epoch 60 \t train_loss: 1.7435\t train_accuracy: 24.9444\t val_loss: 1.9092\t val_accuracy: 24.4259\n","Epoch 61 \t train_loss: 1.7426\t train_accuracy: 25.0483\t val_loss: 1.9087\t val_accuracy: 24.4444\n","Epoch 62 \t train_loss: 1.7419\t train_accuracy: 25.1034\t val_loss: 1.9086\t val_accuracy: 24.4444\n","Epoch 63 \t train_loss: 1.7419\t train_accuracy: 25.1852\t val_loss: 1.9069\t val_accuracy: 24.4974\n","Epoch 64 \t train_loss: 1.7410\t train_accuracy: 25.2484\t val_loss: 1.9050\t val_accuracy: 24.4965\n","Epoch 65 \t train_loss: 1.7396\t train_accuracy: 25.2821\t val_loss: 1.9037\t val_accuracy: 24.7009\n","Epoch 66 \t train_loss: 1.7391\t train_accuracy: 25.3030\t val_loss: 1.9017\t val_accuracy: 24.7811\n","Epoch 67 \t train_loss: 1.7395\t train_accuracy: 25.3578\t val_loss: 1.8993\t val_accuracy: 24.8259\n","Epoch 68 \t train_loss: 1.7379\t train_accuracy: 25.4336\t val_loss: 1.8997\t val_accuracy: 24.8529\n","Epoch 69 \t train_loss: 1.7360\t train_accuracy: 25.5035\t val_loss: 1.8991\t val_accuracy: 24.8953\n","Epoch 70 \t train_loss: 1.7339\t train_accuracy: 25.6593\t val_loss: 1.8967\t val_accuracy: 25.0000\n","Epoch 71 \t train_loss: 1.7332\t train_accuracy: 25.7024\t val_loss: 1.8970\t val_accuracy: 24.9765\n","Epoch 72 \t train_loss: 1.7321\t train_accuracy: 25.7728\t val_loss: 1.8942\t val_accuracy: 25.0000\n","Epoch 73 \t train_loss: 1.7310\t train_accuracy: 25.7921\t val_loss: 1.8928\t val_accuracy: 24.9772\n","Epoch 74 \t train_loss: 1.7307\t train_accuracy: 25.7900\t val_loss: 1.8924\t val_accuracy: 25.1351\n","Epoch 75 \t train_loss: 1.7293\t train_accuracy: 25.8838\t val_loss: 1.8903\t val_accuracy: 25.2296\n","Epoch 76 \t train_loss: 1.7289\t train_accuracy: 25.9480\t val_loss: 1.8900\t val_accuracy: 25.2193\n","Epoch 77 \t train_loss: 1.7285\t train_accuracy: 26.0007\t val_loss: 1.8891\t val_accuracy: 25.3102\n","Epoch 78 \t train_loss: 1.7264\t train_accuracy: 26.0585\t val_loss: 1.8866\t val_accuracy: 25.4416\n","Epoch 79 \t train_loss: 1.7264\t train_accuracy: 26.0987\t val_loss: 1.8856\t val_accuracy: 25.5134\n","Epoch 80 \t train_loss: 1.7260\t train_accuracy: 26.1314\t val_loss: 1.8854\t val_accuracy: 25.5417\n","Epoch 81 \t train_loss: 1.7261\t train_accuracy: 26.1602\t val_loss: 1.8840\t val_accuracy: 25.5967\n","Epoch 82 \t train_loss: 1.7249\t train_accuracy: 26.2539\t val_loss: 1.8804\t val_accuracy: 25.7046\n","Epoch 83 \t train_loss: 1.7235\t train_accuracy: 26.3299\t val_loss: 1.8800\t val_accuracy: 25.7430\n","Epoch 84 \t train_loss: 1.7218\t train_accuracy: 26.3706\t val_loss: 1.8793\t val_accuracy: 25.8466\n","Epoch 85 \t train_loss: 1.7204\t train_accuracy: 26.4796\t val_loss: 1.8797\t val_accuracy: 25.8824\n","Epoch 86 \t train_loss: 1.7195\t train_accuracy: 26.5444\t val_loss: 1.8797\t val_accuracy: 25.8786\n","Epoch 87 \t train_loss: 1.7189\t train_accuracy: 26.6166\t val_loss: 1.8811\t val_accuracy: 25.9132\n","Epoch 88 \t train_loss: 1.7183\t train_accuracy: 26.6871\t val_loss: 1.8804\t val_accuracy: 25.9470\n","Epoch 89 \t train_loss: 1.7178\t train_accuracy: 26.7819\t val_loss: 1.8828\t val_accuracy: 25.9426\n","Epoch 90 \t train_loss: 1.7180\t train_accuracy: 26.8348\t val_loss: 1.8872\t val_accuracy: 25.8889\n","Epoch 91 \t train_loss: 1.7169\t train_accuracy: 26.9315\t val_loss: 1.8872\t val_accuracy: 25.9585\n","Epoch 92 \t train_loss: 1.7154\t train_accuracy: 27.0318\t val_loss: 1.8875\t val_accuracy: 26.0024\n","Epoch 93 \t train_loss: 1.7149\t train_accuracy: 27.0802\t val_loss: 1.8867\t val_accuracy: 26.0096\n","Epoch 94 \t train_loss: 1.7145\t train_accuracy: 27.1358\t val_loss: 1.8854\t val_accuracy: 26.0165\n","Epoch 95 \t train_loss: 1.7141\t train_accuracy: 27.2011\t val_loss: 1.8851\t val_accuracy: 26.1287\n","Epoch 96 \t train_loss: 1.7133\t train_accuracy: 27.3184\t val_loss: 1.8830\t val_accuracy: 26.1806\n","Epoch 97 \t train_loss: 1.7128\t train_accuracy: 27.3672\t val_loss: 1.8812\t val_accuracy: 26.2657\n","Epoch 98 \t train_loss: 1.7118\t train_accuracy: 27.4699\t val_loss: 1.8817\t val_accuracy: 26.3265\n","Epoch 99 \t train_loss: 1.7104\t train_accuracy: 27.5343\t val_loss: 1.8821\t val_accuracy: 26.3412\n","Epoch 100 \t train_loss: 1.7095\t train_accuracy: 27.5846\t val_loss: 1.8849\t val_accuracy: 26.3000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-rQ6ajEiJPJV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"65d7e17b-fc7d-4bbc-e605-f301912fbcc9","executionInfo":{"status":"ok","timestamp":1557478165240,"user_tz":-120,"elapsed":948,"user":{"displayName":"Alessia Ruggeri","photoUrl":"","userId":"01098494933204128714"}}},"source":["### Test loop\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n","\n","test_generator = batch_generator([X_test, A_test, y_test], batch_size=batch_size)\n","\n","for X, A, y in test_generator:\n","  X_batch, A_batch, idx_batch = create_batch_elements(X, A)\n","  test_step([X_batch, A_batch, idx_batch], y)\n","\n","template = 'test_loss: {:.4f}\\t test_accuracy: {:.4f}'\n","print (template.format(test_loss.result(),\n","                       test_accuracy.result()*100))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["test_loss: 1.7228\t test_accuracy: 59.1667\n"],"name":"stdout"}]}]}